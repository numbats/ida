---
title: "ETC5510: Introduction to Data Analysis"
week: "Week 8, part A"
subtitle: "Text analysis"
author: "Nicholas Tierney & Stuart Lee"
email: "ETC5510.Clayton-x@monash.edu"
date: "May 2020"
pdflink: ""
bgimg: "images/bg1.jpg"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/animate.css"
      - "assets/monash-logo.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/styles.css" # small improvements
      - "assets/custom.css" # add your own CSS here!
      - "assets/demo.css" # this should be removed
    self_contained: false 
    seal: false 
    chakra: 'libs/remark-latest.min.js'
    lib_dir: libs
    includes:
      in_header: "assets/custom.html"
    mathjax: "assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: github 
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---
  
```{r titleslide, child="components/titleslide.Rmd"}
```


```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(textdata)
library(tm)

opts_chunk$set(echo = TRUE,   
               out.width = "100%",
               message = FALSE,
               warning = FALSE,
               collapse = TRUE,
               fig.height = 4,
               fig.retina = 2,
               fig.width = 8,
               fig.align = "center",
               dpi = 300,
               cache = FALSE)

as_table <- function(...) knitr::kable(..., format='html', digits = 3)
```

---
class: refresher
# recap

- linear models (are awesome)
- many models

---
# Announcements

- Assignment
- Project
- Peer review marking

---
# Why text analysis?

- Predict Melbourne house prices from realtor descriptions
- Determine the extent of public discontent with train stoppages in Melbourne
- The differences between Darwin's first edition of the Origin of the Species and the 6th edition
- Does the sentiment of posts on Newcastle Jets public facebook page reflect their win/loss record?

---
# Typical Process

1. Read in text
2. Pre-processing: remove punctuation signs, remove numbers, stop words, stem words
3. Tokenise: words, sentences, ngrams, chapters
4. Summarise
5. model

---
# Packages

In addition to `tidyverse` we will be using three other packages today

```{r list-pkgs}
library(tidytext)
library(gutenbergr)
```

---
# Tidytext

- Using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use.
- Learn more at https://www.tidytextmining.com/, by Julia Silge and David Robinson.

---
# What is tidy text?

```{r show-text}
text <- c("This will be an uncertain time for us my love",
          "I can hear the echo of your voice in my head",
          "Singing my love",
          "I can see your face there in my hands my love",
          "I have been blessed by your grace and care my love",
          "Singing my love")

text
```

---
# What is tidy text?

```{r tidy-text-tile}
text_df <- tibble(line = seq_along(text), text = text)

text_df
```

---
# What is tidy text?

.left-code[
```{r unnest-tokens, eval = FALSE}
text_df %>%
  unnest_tokens(output = word,
                input = text,
                token = "words") # default option
```  
]

.right-plot[
```{r unnest-tokens-out, ref.label = 'unnest-tokens', echo = FALSE, out.width = "100%"}

```
]

---
# What is unnesting?

.left-code[
```{r unnest-tokens-chars, eval = FALSE}
text_df %>%
  unnest_tokens(output = word,
                input = text,
                token = "characters")
```  
]

.right-plot[
```{r unnest-tokens-chars-out, ref.label = 'unnest-tokens-chars', echo = FALSE}

```
]

---
# What is unnesting - ngrams length 2

.left-code[
```{r unnest-tokens-ngram-2, eval = FALSE}
text_df %>%
  unnest_tokens(output = word,
                input = text,
                token = "ngrams",
                n = 2)
```  
]

.right-plot[
```{r unnest-tokens-ngram-2-out, ref.label = 'unnest-tokens-ngram-2', echo = FALSE}

```
]

---
# What is unnesting - ngrams length 3

.left-code[
```{r unnest-tokens-ngram-3, eval = FALSE}
text_df %>%
  unnest_tokens(output = word,
                input = text,
                token = "ngrams",
                n = 3)
```  
]

.right-plot[
```{r unnest-tokens-ngram-3-out, ref.label = 'unnest-tokens-ngram-3', echo = FALSE, out.width = "100%"}

```
]

---
class: transition

# Your Turn:

Complete "8a-tokenizing.Rmd"

---
class: transition

# Analyzing user reviews for Animal Crossing: New Horizons

---
# About the data

- User and critic reviews for the game [Animal Crossing](https://www.nintendo.com/games/detail/animal-crossing-new-horizons-switch/) scraped from Metacritc

- This data comes from a [#TidyTuesday challenge](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-05/readme.md). 



---
# What do the user reviews look like?

```{r, echo = TRUE}
acnh_user_reviews <- read_tsv(here::here("slides/data/acnh_user_reviews.tsv"))
glimpse(acnh_user_reviews)
```


---
# Let's look at the grade distrubtion

```{r review-grades, echo = FALSE}
acnh_user_reviews %>% 
  count(grade) %>% 
  ggplot(aes(x = grade, y = n )) +
  geom_col()
```


---
# Read a few of the positive reviews

.left_code[
```{r pos-reviews, echo = TRUE, eval = FALSE}
set.seed(1999)
acnh_user_reviews %>% 
  filter(grade > 8) %>% 
  sample_n(3) %>% 
  pull(text)
```
]

.pull_right[
```{r ref.label="pos-reviews", eval = TRUE, echo = FALSE}
```
]


---
# And some negative reviews

.left_code[
```{r neg-reviews, echo = TRUE, eval = FALSE }
set.seed(2099)
acnh_user_reviews %>% 
  filter(grade == 0) %>% 
  sample_n(3) %>% 
  pull(text)
```
]

.pull_right[
```{r ref.label="neg-reviews", eval = TRUE, echo = FALSE}
```
]

---
# Looks like the scraping is messed up a bit

Long reviews are compressed from the scraping procedure...
```{r,}
acnh_user_reviews_parsed <- acnh_user_reviews %>% 
  mutate(text = str_remove(text, "Expand$"))
```

We will remove these characters from the text..

---
# Tidy up the reviews!

```{r unnest-tokens-acnh}
user_reviews_words <- acnh_user_reviews_parsed %>%
  unnest_tokens(output = word, input = text)

user_reviews_words
```

---
# Distribution of words per review?

```{r word-histogram, out.width="90%"}
user_reviews_words %>% 
  count(user_name) %>% 
  ggplot(aes(x = n)) +
  geom_histogram()
```

---
# What are the most common words?

```{r common-words}
user_reviews_words %>%
  count(word, sort = TRUE)
```

---
# Stop words

- In computing, stop words are words which are filtered out before or after processing of natural language data (text).
- They usually refer to the most common words in a language, but there is not a single list of stop words used by all natural language processing tools.

---
# English stop words

```{r eng-stopwords}
get_stopwords()
```

---
# Spanish stop words

```{r spanish-stopwords}
get_stopwords(language = "es")
```

---
# Various lexicons

See `?get_stopwords` for more info.

```{r other-lexicons}
get_stopwords(source = "smart")
```

---
# What are the most common words?

```{r repeat}
user_reviews_words %>%
  count(word, sort = TRUE)
```

---
# What are the most common words?

```{r stopwords-anti-join}
stopwords_smart <- get_stopwords(source = "smart")

user_reviews_words %>%
  anti_join(stopwords_smart) 
```

---
## Aside: the anti-join

- A type of filtering join, will return all rows on the left when there
are no matches on the right
- Only keeps columns on the left 

---
## As a picture

```{r animate-anti-join, echo = FALSE, out.width = "50%"}
include_graphics("gifs/anti-join.gif")
```

---
# What are the most common words?

```{r stopwords-anti-join-complete}
user_reviews_words %>%
  anti_join(stopwords_smart) %>%
  count(word, sort = TRUE) 
```

---
# What are the most common words?

```{r gg-common-words, eval=FALSE}
user_reviews_words %>%
  anti_join(stopwords_smart) %>%
  count(word) %>%
  arrange(-n) %>%
  top_n(20) %>%
  ggplot(aes(fct_reorder(word, n), n)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(title = "Frequency of words in user reviews",
       subtitle = "",
       y = "",
       x = "")
```

---

```{r gg-common-words-out, ref.label = 'gg-common-words', echo = FALSE, out.width = "100%"}
```

---
class: transition

# Your turn: 

Complete "8a-stopwords.Rmd"

---
class: transition

# Sentiment Analysis

---
# Sentiment analysis

- One way to analyze the sentiment of a text is to consider the text as a combination of its individual words 

- and the sentiment content of the whole text as the sum of the sentiment content of the individual words

---
# Sentiment lexicons

.pull-left[
```{r show-sentiment-afinn}
get_sentiments("afinn")
```
]

.pull-right[
```{r show-sentiment-bing}
get_sentiments("bing")
```
]

---
# Sentiment lexicons

.pull-left[
```{r show-sentiment-bing2}
get_sentiments(lexicon = "bing")
```
]

.pull-right[
```{r show-sentiment-loughran}
get_sentiments(lexicon = "loughran")
```
]

---
# Sentiments in the reviews

```{r sentiment-reviews}
sentiments_bing <- get_sentiments("bing")

user_reviews_words %>%
  inner_join(sentiments_bing) %>%
  count(sentiment, word, sort = TRUE) 
```

---
# Visualising sentiments

```{r gg-sentiment, echo=TRUE, message=FALSE, eval = FALSE}
user_reviews_words %>%
  inner_join(sentiments_bing) %>%
  count(sentiment, word, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free") +
  theme_minimal() +
  labs(
    title = "Sentiments in user reviews",
    x = ""
  )
```

---
# Visualising sentiments

```{r gg-sentiment2, echo = FALSE}
user_reviews_words %>%
  inner_join(sentiments_bing) %>%
  count(sentiment, word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free") +
  theme_minimal() +
  labs(
    title = "Sentiments in user reviews",
    x = ""
  )
```


---

# Common words over grades 

```{r common-user-words-stop}
user_reviews_words %>%
  anti_join(stopwords_smart) %>%
  count(grade, word, sort = TRUE) 
```

---
# Common review words by grade - With stop words:

```{r common-user-words}
user_reviews_words %>%
  count(grade, word, sort = TRUE)
```

---
class: transition

# Your turn: 

Complete "8a-sentiment.Rmd"

---
# What is a document about?

## How do we measure the importance of a word to a document in a collection of documents?

i.e a novel in a collection of novels or a review in a set of reviews...

We combine the following statistics:

- Term frequency
- Inverse document frequency

---

# Term frequency

The raw frequency of a word $w$ in a document $d$. It is a function of the word and the document. 

$$tf(w, d) = \frac{\text{count of w in d}}{\text{total count in d}} $$

---
# Term frequency

For our reviews a document is a single user's review.

```{r doc-example}
document <- user_reviews_words %>% 
    anti_join(stopwords_smart) %>% 
    filter(user_name == "Discoduckasaur")
document
```


---
# Term frequency

The term frequency for each word is the number of times that word occurs
divided by the total number of words in the document.

.pull-left[
```{r tf}
tbl_tf <- document %>% 
  count(word, sort = TRUE) %>% 
  mutate(tf = n / sum(n))
```
]

.pull-right[

```{r tf-print}
tbl_tf %>% 
  arrange(desc(tf))
```
]
---

## Inverse-document frequency

The inverse document frequency tells how common or rare a word is accross a collection of documents. It is a function of a word $w$, and the collection of documents $\mathcal{D}$.

$$idf(w, \mathcal{D}) = \log{\left(\frac{\text{size of } \mathcal{D}}{\text{number of documents that contain } w}\right)}$$
---

## Inverse document frequency

For the reviews data set, our collection is all the reviews. You could
compute this in a somewhat roundabout as follows:

```{r idf}
tbl_idf <- user_reviews_words %>% 
    anti_join(stopwords_smart) %>%
    mutate(collection_size = n_distinct(user_name)) %>% 
    group_by(collection_size, word) %>% 
    summarise(times_word_used = n_distinct(user_name)) %>% 
    mutate(freq = collection_size / times_word_used,
           idf = log(freq)) 
arrange(tbl_idf, idf)
```

---
## Putting it together term frequency, inverse document frequency

Multiply tf and idf together. This is a function of a word $w$, a document $d$,
and the collection of documents $\mathcal{D}$:

$$ tf\_idf(w, d, \mathcal{D}) = tf(w,d) \times idf(w, \mathcal{D})$$
High value of $tf\_idf$ that a word has a high frequency within a document
but is quite rare over all documents. Likewise if a word occurs in a lot
of documents idf will be close to zero, so $tf\_idf$ will be small.

---
## Putting it together, tf-idf 

We illustrate the example for a single user review:

```{r tf-idf}
tbl_tf %>% 
    left_join(tbl_idf) %>% 
    select(word, tf, idf) %>% 
    mutate(tf_idf = tf * idf) %>% 
    arrange(desc(tf_idf))
```
---
# Calculating tf-idf: Perhaps not that exciting...

Instead of rolling our own, we can use `tidytext`

```{r calc-tf-idf}
user_reviews_counts <- user_reviews_words %>%
      anti_join(stopwords_smart) %>% 
      count(user_name, word, sort = TRUE) %>% 
      bind_tf_idf(term = word, document = user_name, n = n)

user_reviews_counts
```

---

# What words were important to (a sample of) users that had positive reviews?

```{r gg-tf-idf, echo=FALSE,message=FALSE}
pos_reviews <- acnh_user_reviews_parsed %>% 
    select(user_name, grade) %>% 
    filter(grade > 8) %>% 
    sample_n(3)

user_reviews_counts_pos <- user_reviews_counts %>%
  inner_join(pos_reviews, by = "user_name") 

user_reviews_counts_pos %>% 
  group_by(user_name) %>%
  top_n(10, wt = tf_idf) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(word, tf_idf), tf_idf, fill = user_name)) +
  geom_col(show.legend = FALSE) + 
  coord_flip() +
  facet_wrap(~user_name, ncol = 1, scales = "free") +
  scale_y_continuous() +
  theme_minimal() +
  labs(x = NULL, y = "tf-idf")
```

---
class: transition

# Your Turn

Complete "8a-animal-crossing.Rmd"
  - This time we will look at critics reviews

---
# Lab exercise (bonus!)

Text Mining with R has an example comparing historical physics textbooks:  *Discourse on Floating Bodies* by Galileo Galilei, *Treatise on Light* by Christiaan Huygens, *Experiments with Alternate Currents of High Potential and High Frequency* by Nikola Tesla, and *Relativity: The Special and General Theory* by Albert Einstein. All are available on the Gutenberg project. 

Work your way through the [comparison of physics books](https://www.tidytextmining.com/tfidf.html#a-corpus-of-physics-texts). It is section 3.4.

---
# Thanks

- Dr. Mine Ã‡etinkaya-Rundel
- Dr. Julia Silge: https://github.com/juliasilge/tidytext-tutorial and
https://juliasilge.com/blog/animal-crossing/ 
- Dr. Julia Silge and Dr. David Robinson: https://www.tidytextmining.com/
