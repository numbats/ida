---
title: "ETC1010: Introduction to Data Analysis"
week: "Week 10, part A"
subtitle: "Regression and Decision Trees"
author: "Nicholas Tierney & Stuart Lee"
email: "nicholas.tierney@monash.edu"
date: "May 2020"
pdflink: ""
bgimg: "images/bg1.jpg"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/animate.css"
      - "assets/monash-logo.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/styles.css" # small improvements
      - "assets/custom.css" # add your own CSS here!
      - "assets/demo.css" # this should be removed
    self_contained: false 
    seal: false 
    chakra: 'libs/remark-latest.min.js'
    lib_dir: libs
    includes:
      in_header: "assets/custom.html"
    mathjax: "assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: github 
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---
  
```{r setup, include=FALSE}
library(tidyverse)
library(tibble)
library(vctrs)
library(knitr)
library(kableExtra)
library(countdown)
library(knitr)
library(lubridate)
library(gridExtra)
library(plotly)
library(broom)
opts_chunk$set(echo = TRUE,   
               message = FALSE,
               warning = FALSE,
               collapse = TRUE,
               fig.height = 4,
               fig.width = 8,
               fig.retina = 2,
               out.width = "100%",
               fig.align = "center",
               cache = FALSE)
```


  
```{r titleslide, child="components/titleslide.Rmd"}
```

---
# recap

- networks

---
# Overview

- What is a regression tree?
- How is it computed?
- Deciding when its a good fit
- Comparison with linear models
- How a classification tree differs from a regression tree?


---
# Example

Here's a synthetic data set for illustration. Just making up a function to simulate some data to play with.

```{r simulated-data, echo = FALSE}
set.seed(2020 - 05 - 21)
x <- sort(runif(100) - 0.5)
df <- data.frame(x,
                 y = 10 * c(x[1:50] ^ 2,
                            x[51:75] * 2,
                            -x[76:100] ^ 2) + rnorm(100) * 0.5)

ggplot(df, aes(x = x, y = y)) + geom_point()
```

---
# Aims

- We want to be able to predict the Y values

---
# Using a linear model

```{r show-lm}
df_lm <- lm(y ~ x, df)
```


```{r lm, echo = FALSE}
ggplot(df, aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

---
# Looking at the residuals

```{r resid, echo = FALSE}
df_lm_aug <- augment(df_lm)
ggplot(df_lm_aug, aes(x = .fitted,
                      y = .resid)) + 
  geom_point()
```

---
# Using rpart

```{r echo=TRUE}
library(rpart)
library(broom)
library(broomstick)
df_rp <- rpart(y~x, data=df)
```


```{r echo=TRUE}
df_rp_aug <- augment(df_rp)
ggplot(df_rp_aug,
       aes(x = .fitted,
           y = .resid)) + 
  geom_point()

bind_rows(lm = df_lm_aug,
          rpart = df_rp_aug,
          .id = "model") %>% 
  ggplot(aes(x = .fitted,
             y = .resid,
             colour = model)) + 
  geom_point()
```

---
# What the output of a linear model looks like

```{r show-lm, echo = FALSE}
df_lm
```

---
# What the output of a decision tree looks like

```{r show-rpart, echo = FALSE}
df_rpart
```


---
# Understanding rpart

```{r echo=TRUE}
df_rp
```

**There's a lot going on here, let's break it down.**

---
# Plot the model

```{r}
library(rpart.plot)
rpart.plot(df_rp)
```

---
# Plot the model on the data

This is how the data is split:

```{r show-split, echo = FALSE}
splt <- as_tibble(df_rp$splits) %>% 
  rowid_to_column(var = "order") %>%
  select(index, order)
ggplot(df, aes(x = x, y = y)) + geom_point() +
  geom_vline(data = splt,
             aes(xintercept = index, 
                 colour = factor(order)),
             linetype = 2) +
  geom_text(data = splt,
            aes(x = index, y = max(df$y), label = order),
            nudge_x = 0.02) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
  scale_colour_viridis_d() +
  theme_bw() +
  theme(legend.position = "none")
```

---
# This is how the model looks:

```{r}
df <- df %>% 
  mutate(bucket = cut(x, breaks=c(min(x)-0.1, splt$index, max(x)))) 
df_pred <- df %>% 
  group_by(bucket) %>%
  mutate(pred = mean(y)) %>%
  arrange(x)
ggplot(df_pred) + 
  geom_point(aes(x=x, y=y)) +
  geom_line(aes(x=x, y=pred), colour="hotpink", size=1.5) +
    geom_vline(data=splt, aes(xintercept = index, colour=factor(order)), 
             linetype=2) + 
  scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1)) +
  scale_colour_viridis_d() +
  theme_bw() +
  theme(legend.position="none")
```

---
# So what is going on?

- A linear model asks "What line fits through these points, to minimise the error"?
- A decision tree model asks "How can I best break the data into segments" 

---
# A linear model: draws the line of best fit

```{r echo = FALSE}
ggplot(data = df_lm_aug, 
            mapping = aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = .fitted), color = "steelblue", size = 1) +
  geom_segment(mapping = aes(xend = x, 
                             yend = .fitted), 
               color = "grey50",
               alpha = 0.5)
```

---
# A regression tree: segments the data to reduce some error

```{r}
ggplot(df, aes(x = x, 
               y = y)) + 
  geom_point() +
  geom_vline(data = splt,
             aes(xintercept = index, 
                 colour = factor(order)),
             linetype = 2) +
  geom_text(data = splt,
            aes(x = index, y = max(df$y), label = order),
            nudge_x = 0.02) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
  scale_colour_viridis_d() +
  theme_bw() +
  theme(legend.position = "none")
```


---
# Regression trees

- Regression trees recursively partition the data, and use the average response value of each partition as the model estimate
- It is a computationally intensive technique, involves examining ALL POSSIBLE partitions. 
- The BEST partition by optimizing some criteria
- For regression, with a quantitative response variable, the criteria is called ANOVA:

$$SS_T-(SS_L+SS_R)$$
where $SS_T = \sum (y_i-\bar{y})^2$, and $SS_L, SS_R$ are the equivalent values for the two subsets created by partitioning.

---
# Stopping rules

- Its an algorithm. Why did it stop at 7 groups?
- Stopping rules are needed, else the algorithm will keep fitting until every observation is in its own group.
- Control parameters set stopping points:
   + minsplit: minimum number of points in a node that algorithm is allowed to split
   + minbucket: minimum number of points in a terminal node
- In addition, we can also look at the change in value of $SS_T-(SS_L+SS_R)$ at each split, and if the change is too *small*, stop. To decide on a suitable value for *small* a cross-validation procedure is used.

---
#  Below are the controls for the fit on the example data:

```{r}
str(df_rp$control)
```

If you change these options and re-fit, the model will change. Here we reduce the `minbucket` parameter. 

```{r echo=TRUE}
df_rp <- rpart(y~x, data=df, 
  control = rpart.control(minsplit=10))
df_rp
```


---
# This yields a more complex model.

```{r fig.width=8}
df_pred2 <- df %>% 
  mutate(pred = predict(df_rp, df))
p1 <- ggplot(df_pred) + 
  geom_point(aes(x=x, y=y)) +
  geom_line(aes(x=x, y=pred), colour="hotpink", size=1.5) +
  scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1)) +
  theme_bw() + ggtitle("Old model")
p2 <- ggplot(df_pred2) + 
  geom_point(aes(x=x, y=y)) +
  geom_line(aes(x=x, y=pred), colour="orange", size=1.5) +
  scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1)) +
  theme_bw() + ggtitle("New model")
grid.arrange(p1, p2, ncol=2)
```

---
# What's computed?

Illustration showing the calculations made to decide on the first partition.

```{r}
sst <- var(df$y)*(nrow(df)-1)
compute_anova <- function(left, right) {
  ssl <- var(left$y)*(nrow(left)-1)
  if (nrow(left) == 1) {
    ssl <- 1
}
  ssr <- var(right$y)*(nrow(right)-1)
  if (nrow(right) == 1) {
    ssr <- 1
  }
  av <- sst - (ssl+ssr)
  return(av)
}
aov_f <- data.frame(x=df$x[-1], f=df$y[-1])
for (i in 2:nrow(df)) {
  left <- df[1:(i-1),]
  right <- df[i:nrow(df),]
  aov_f$x[i-1] <- mean(df$x[c(i-1, i)])
  aov_f$f[i-1] <- compute_anova(left, right)
}
p1 <- ggplot(df, aes(x=x, y=y)) + geom_point(alpha=0.5) + scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1))
p2 <- ggplot(data=aov_f) +
  geom_line(aes(x=x, y=f), colour="hotpink") +
    geom_vline(xintercept = df_rp$splits[1,4], colour="hotpink", linetype=2)
grid.arrange(p1, p2, ncol=1)
```

---
# Residuals

```{r}
df_rp <- rpart(y~x, data=df)
df_rp_aug <- cbind(df, e=residuals(df_rp))
ggplot(df_rp_aug, aes(x=x, y=e)) + geom_point() +
  ylab("residuals") + scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1))
```

---
# Goodness of fit

```{r echo=TRUE}
gof <- printcp(df_rp, digits=3)
```

The relative error is $1-R^2$. For this example, after 6 splits it is `r gof[6,3]`. So $R^2=$ `r 1-gof[6,3]`. 

```{r eval=FALSE}
1-sum(df_rp_aug$e^2)/sum((df$y-mean(df$y))^2)
```

---
# Strengths and weaknesses

- There are no parametric assumptions underlying partitioning methods
- Also means that there is not a nice formula for the model as a result, or inference about populations available
- By minimizing sum of squares (ANOVA) we are forcing the partitions to have relatively equal variance. The method could be influenced by outliers, but it would be isolating the effect to one partition.
- Because it operates on single variables, it can efficiently handle missing values. 

---
# Your turn: compute a regression tree

Here is a small data set. Manually compute a regression tree model for the data. Sketch the model.

```{r eval=FALSE}
d <- tibble(x=c(1, 2, 3, 4, 5), y=c(10, 12, 5, 4, 3))
d
ggplot(d, aes(x=x, y=y)) + 
  geom_???()
```

```{r eval=FALSE, echo=FALSE}
sst <- var(d$y)*(nrow(???)-1)
compute_anova(???,???)
```

---
# Classification trees

When the response is categorical, the model is called a classification tree. The criteria for making the splits changes also. There are a number of split criteria commonly used. If we consider a binary response ($y=0, 1$), and $p$ is the proportion of observations in class $1$.

- Gini: $2p(1-p)$
- Entropy: $-p(\log_e p)-(1-p)\log_e(1-p)$

Which rewards splits where the observations are all one class.

---
# Lab exercise

- OECD PISA, what factors affect reading scores?
- 15 year old standardised test scores, Australia, 2015
- Response: math
- Predictors: gender, anxtest, wealth, math_time, books, tvs
