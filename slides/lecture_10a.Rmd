---
title: "ETC1010: Introduction to Data Analysis"
week: "Week 10, part A"
subtitle: "Regression and Decision Trees"
author: "Nicholas Tierney & Stuart Lee"
email: "nicholas.tierney@monash.edu"
date: "May 2020"
pdflink: ""
bgimg: "images/bg1.jpg"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/animate.css"
      - "assets/monash-logo.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/styles.css" # small improvements
      - "assets/custom.css" # add your own CSS here!
      - "assets/demo.css" # this should be removed
    self_contained: false 
    seal: false 
    chakra: 'libs/remark-latest.min.js'
    lib_dir: libs
    includes:
      in_header: "assets/custom.html"
    mathjax: "assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: github 
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---
  
```{r setup, include=FALSE}
library(tidyverse)
library(tibble)
library(vctrs)
library(knitr)
library(kableExtra)
library(countdown)
library(knitr)
library(lubridate)
library(gridExtra)
library(plotly)
library(broom)
library(broomstick)

opts_chunk$set(echo = TRUE,   
               message = FALSE,
               warning = FALSE,
               collapse = TRUE,
               fig.height = 4,
               fig.width = 8,
               fig.retina = 2,
               out.width = "100%",
               fig.align = "center",
               cache = FALSE)
```

```{r funs, echo = FALSE}
draw_split <- function(split){

df_add_sst <- df %>% 
  mutate(x_split = if_else(x <= split, "left", "right")) %>% 
  group_by(x_split) %>% 
  mutate(y_bar = mean(y),
         y_slope = 0) 

ggplot(df_add_sst, 
       aes(x = x, 
           y = y)) + 
  geom_point() +
  geom_vline(xintercept = split, linetype = 2) +
  geom_line(aes(y = y_bar), colour = "salmon") + 
  geom_segment(mapping = aes(xend = x, 
                             yend = y_bar), 
               color = "grey50",
               alpha = 0.5) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1))
}
```

  
```{r titleslide, child="components/titleslide.Rmd"}
```

---
# recap

- networks

---
# Overview

- What is a regression tree?
- How is it computed?
- Deciding when its a good fit
  - rmse
- Comparison with linear models
- Using multiple variables
- Next class:
  - How a classification tree differs from a regression tree?


---
# Example

```{r simulated-data, echo = FALSE}
set.seed(2020 - 05 - 21)
x <- sort(runif(100) - 0.5)
df <- data.frame(x,
                 y = 10 * c(x[1:50] ^ 2,
                            x[51:75] * 2,
                            -x[76:100] ^ 2) + rnorm(100) * 0.5)

ggplot(df, aes(x = x, y = y)) + geom_point()
```

---
# Let's predict Y using a linear model

```{r show-lm}
df_lm <- lm(y ~ x, df)
```


```{r lm, echo = FALSE}
ggplot(df, aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

---
# Assessing model fit

- Look at residuals
- Look at mean square error

---
# Looking at the residuals: this is bad!

```{r resid, echo = FALSE}
df_lm_aug <- augment(df_lm)
ggplot(df_lm_aug, aes(x = x,
                      y = .resid)) + 
  geom_point()
```

It basically looks like the data!

---
# Looking at the Mean square error (MSE)

This is another way to assess a model, it is like taking the average amount of error in the model.

$$
MSE(y) = \frac{\sum_{i = 1}^{i = N} (y_i - \hat{y}_i)^2}{N}
$$
In R code:

```{r mse}
mse <- function(model){
  mod_aug <- augment(model)
  mod_aug %>% 
    mutate(res_2 = .resid^2) %>% 
    summarise(mse = mean(sum(res_2))) %>% 
    pull(mse)
}

mse(df_lm)
```

---
# Let's use a different model: "rpart"

```{r echo=TRUE}
library(rpart)
# df_lm <- lm(y~x, data=df) - similar to lm! But rpart.
df_rp <- rpart(y~x, data=df)

```

--

```{r echo=FALSE}
splt <- as_tibble(df_rp$splits) %>% 
  rowid_to_column(var = "order") %>%
  select(index, order)

df_rp_aug <- augment(df_rp)
ggplot(df_rp_aug,
       aes(x = x,
           y = y)) + 
  geom_point() +
  geom_line(aes(y = .fitted), colour = "salmon", size = 2)


```

---
# Comparing lm vs rpart: Predictions

```{r compare, echo = FALSE}
df_aug_both <- 
bind_rows(lm = df_lm_aug,
          rpart = df_rp_aug,
          .id = "model") 

ggplot(df_aug_both,
       aes(x = x,
           y = y)) + 
  geom_point() +
  geom_line(aes(y = .fitted,
                colour = model), 
            size = 1)
```


---
# Comparing lm vs rpart: MSE

```{r}
# linear model
mse(df_lm)

# rpart model
mse(df_rp)
```

--

The rpart model is much lower! 

--

We can look at the residuals plotted against the values of x to get an idea

---
# Comparing lm vs rpart: residuals


```{r echo=FALSE}

ggplot(df_aug_both, aes(x = x,
             y = .resid,
             colour = model)) + 
  geom_point()
```

---
# Comparing lm vs rpart: output

```{r show-lm-putput, echo = FALSE}
df_lm
```

---
# Comparing lm vs rpart: output

```{r show-rpart, echo = FALSE}
df_rp
```

---
# So what is going on?

- A linear model asks "What line fits through these points, to minimise the error"?
- A decision tree model asks "How can I best break the data into segments, to minimize some error?" 

---
# A linear model: draws the line of best fit

```{r echo = FALSE}
ggplot(data = df_lm_aug, 
            mapping = aes(x = x, y = y)) +
  geom_point() + 
  geom_line(aes(y = .fitted), color = "steelblue", size = 1) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
  geom_segment(mapping = aes(xend = x, 
                             yend = .fitted), 
               color = "grey50",
               alpha = 0.5)
```

---
# A regression tree: segments the data to reduce mean error

```{r echo = FALSE}
ggplot(df, aes(x = x, 
               y = y)) + 
  geom_point() +
  geom_vline(aes(xintercept = splt$index[1], 
                 colour = "salmon"),
             linetype = 2) +
  geom_text(data = splt[1, ],
            aes(x = index, y = max(df$y), label = order),
            nudge_x = 0.02) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
  scale_colour_viridis_d() +
  theme(legend.position = "none")
```


---
# Regression trees

- Regression trees recursively partition the data, and use the average response value of each partition as the model estimate
- It is a computationally intense technique that examines all possible partitions, and choosing the BEST partition by optimizing some criteria
- For regression, with a quantitative response variable, the criteria to maximise is called ANOVA:

$$SS_T-(SS_L+SS_R)$$
where $SS_T = \sum (y_i-\bar{y})^2$, and $SS_L, SS_R$ are the equivalent values for the two subsets created by partitioning.

---
# Break down: What is $SS_T = \sum (y_i-\bar{y})^2$ ?

```{r echo = FALSE}
df_add_sst <- df %>% 
  mutate(y_bar = mean(y),
         y_slope = 0)

ggplot(df_add_sst, 
       aes(x = x, 
           y = y)) + 
  geom_point() +
  geom_line(aes(y = y_bar), colour = "salmon") +
  geom_segment(mapping = aes(xend = x, 
                             yend = y_bar), 
               color = "grey50",
               alpha = 0.5) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1))
```

---
# Break down: What is $SS_T = \sum (y_i-\bar{y})^2$ ?

```{r what-is-error, echo = FALSE}
ggplot(df_add_sst, 
       aes(x = x, 
           y = y)) + 
  geom_segment(mapping = aes(xend = x, 
                             yend = y_bar), 
               color = "grey50",
               alpha = 0.5) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1))
```

---

# $SS_L$ $SS_R$? Choose a point, compare the left and right


```{r ssl-ssr-1, echo = FALSE}
draw_split(-0.4)
```

---

# $SS_L$ $SS_R$? Choose a point, compare the left and right


```{r ssl-ssr-2, echo = FALSE}
draw_split(-0.2)
```

---

# $SS_L$ $SS_R$? Choose a point, compare the left and right


```{r ssl-ssr-3, echo = FALSE}
draw_split(0)
```

---

# $SS_L$ $SS_R$? Choose a point, compare the left and right

```{r ssl-ssr-4, echo = FALSE}
draw_split(0.2)
```

---

# $SS_L$ $SS_R$? Choose a point, compare the left and right

```{r fun-ssl-ssr-5, echo = FALSE}
draw_split(0.4)
```


---
# Across all values of x?

```{r echo = FALSE}
sst <- var(df$y)*(nrow(df)-1)
compute_anova <- function(left, right) {
  ssl <- var(left$y)*(nrow(left)-1)
  if (nrow(left) == 1) {
    ssl <- 1
}
  ssr <- var(right$y)*(nrow(right)-1)
  if (nrow(right) == 1) {
    ssr <- 1
  }
  av <- sst - (ssl+ssr)
  # av <- (ssl+ssr)
  return(av)
}
aov_f <- data.frame(x=df$x[-1], f=df$y[-1])
for (i in 2:nrow(df)) {
  left <- df[1:(i-1),]
  right <- df[i:nrow(df),]
  aov_f$x[i-1] <- mean(df$x[c(i-1, i)])
  aov_f$f[i-1] <- compute_anova(left, right)
}
```


```{r echo = FALSE, fig.height = 5}
p1 <- ggplot(df, aes(x=x, y=y)) + geom_point(alpha=0.5) + scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1))
p2 <- ggplot(data=aov_f) +
  geom_line(aes(x=x, y=f) , colour="hotpink") +
    geom_vline(xintercept = df_rp$splits[1,4], colour="hotpink", linetype=2)+
   scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1))
grid.arrange(p1, p2, ncol=1)
```

---
# And if we repeated this within each split

This is how the data is split:

```{r show-split, echo = FALSE}
ggplot(df, aes(x = x, y = y)) + geom_point() +
  geom_vline(data = splt,
             aes(xintercept = index, 
                 colour = factor(order)),
             linetype = 2) +
  geom_text(data = splt,
            aes(x = index, y = max(df$y), label = order),
            nudge_x = 0.02) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1)) +
  scale_colour_viridis_d() +
  theme_bw() +
  theme(legend.position = "none")
```

---
# We can represent these splits in a tree format:

```{r}
library(rpart.plot)
rpart.plot(df_rp)
```

---
# The model predictions with the splits

```{r model-pred, echo = FALSE}
df <- df %>% 
  mutate(bucket = cut(x, breaks=c(min(x)-0.1, splt$index, max(x)))) 
df_pred <- df %>% 
  group_by(bucket) %>%
  mutate(pred = mean(y)) %>%
  arrange(x)
ggplot(df_pred) + 
  geom_point(aes(x=x, y=y)) +
  geom_line(aes(x=x, y=pred), colour="hotpink", size=1.5) +
    geom_vline(data=splt, aes(xintercept = index, colour=factor(order)), 
             linetype=2) + 
  scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1)) +
  scale_colour_viridis_d() +
  theme_bw() +
  theme(legend.position="none")
```

---
# Your turn: compute a regression tree

Using the small data set, manually compute a regression tree model for the data. Sketch the model.

```{r eval=FALSE}
d <- tibble(x=c(1, 2, 3, 4, 5), y=c(10, 12, 5, 4, 3))
d
ggplot(d, aes(x=x, y=y)) + 
  geom_???()
```

```{r eval=FALSE, echo=FALSE}
sst <- var(d$y)*(nrow(???)-1)
compute_anova(???,???)
```

---
# Stopping rules

- Its an algorithm, and it has to know when to stop.
- Why did it stop at 7 terminal nodes?
- Stopping rules are needed, else the algorithm will keep fitting until every observation is in its own group.
- Control parameters set stopping points:
   + **minsplit**: minimum number of points in a node that algorithm is allowed to split
   + **minbucket**: minimum number of points in a terminal node
- We can also look at the change in value of $SS_T-(SS_L+SS_R)$ at each split, and if the change is too *small*, stop.

---
# You can change the options to fit a different model

An re-fit, the model will change. Here we reduce the `minbucket` parameter. 

```{r echo=TRUE}
df_rp_m10 <- rpart(y~x, data=df, 
                        control = rpart.control(minsplit = 2)) #<<
```


---
# This yields a (slightly) more complex model.

```{r echo = FALSE, fig.width=8}
df_rp_m10_aug <- augment(df_rp_m10)

updated_rpart <- bind_rows(
  old = df_rp_aug,
  new = df_rp_m10_aug,
  .id = "model"
)

ggplot(updated_rpart,
       aes(x = x, 
                 y = y)) +
  geom_point() +
  geom_line(aes(y = .fitted,
                colour = model)) +
  scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1))
```

---
# Beyond one variable

- So far we have only considered cases with one explanatory variable:

```r
rpart(y ~ x)
```

- When given multiple variables, a decision tree will **only use variables that provide the best splits**
- This means that we can identify variables that are important for predicting an outcome.
- This is called "Variable importance"

---
# Variable importance

- After calculating all the potential splits, each variable is given an importance value, that is typically based on the number of times it was used in splitting, and the order in the splits
- The earlier the split, the more important the variable.
- These "importance values" are usually scaled to sum to 100
- But the numbers themselves are arbitrary
- Let's explore this in the next exercise, "10-exercise-lab-1.Rmd"

---
class: transition
# Wrapping up

---
# Strengths

- There are no parametric assumptions underlying partitioning methods
- Can handle data of unusual shapes and sizes
- Can identify unusual groups of data
- Provides a tree based graphic that is fun to interpret
- Has an efficient heuristic of handling missing values. 
- The method could be influenced by outliers, but it would be isolating them to one partition.
  
---
# Weaknesses 

- Doesn't really handle data that is linear very well
- Can require tuning parameters to get good model fit

- Also means that there is not a nice formula for the model as a result, or inference about populations available
  - E.g., You can't say things like: "For every one unit increase in weight, we expect height to increase by XX amount".

---
# Next class: 

- Classification trees
