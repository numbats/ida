<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ETC1010: Introduction to Data Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Nicholas Tierney &amp; Stuart Lee" />
    <script src="libs/header-attrs-2.1/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
    <link href="libs/font-awesome-animation-1.0/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome-5.0.13/js/fontawesome-all.min.js"></script>
    <!--
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> 
    -->
    <link rel="icon" href="images/favicon.ico"  type='image/x-icon'/>    
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-logo.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="assets/demo.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

  




  


&lt;!-- background-color: #006DAE --&gt;
&lt;!-- class: middle center hide-slide-number --&gt;

&lt;div class="shade_black"  style="width:60%;right:0;bottom:0;padding:10px;border: dashed 4px white;margin: auto;"&gt;
&lt;i class="fas fa-exclamation-circle"&gt;&lt;/i&gt; These slides are viewed best by Chrome and occasionally need to be refreshed if elements did not load properly. See &lt;a href=/&gt;here for PDF &lt;i class="fas fa-file-pdf"&gt;&lt;/i&gt;&lt;/a&gt;.
&lt;/div&gt;

&lt;br&gt;

.white[Press the **right arrow** to progress to the next slide!]

---

background-image: url(images/bg1.jpg)
background-size: cover
class: hide-slide-number split-70 title-slide
count: false

.column.shade_black[.content[

&lt;br&gt;

# .monash-blue.outline-text[ETC1010: Introduction to Data Analysis]

&lt;h2 class="monash-blue2 outline-text" style="font-size: 30pt!important;"&gt;Week 10, part A&lt;/h2&gt;

&lt;br&gt;

&lt;h2 style="font-weight:900!important;"&gt;Regression and Decision Trees&lt;/h2&gt;

.bottom_abs.width100[

Lecturer: *Nicholas Tierney &amp; Stuart Lee*

Department of Econometrics and Business Statistics

<span>&lt;i class="fas  fa-envelope faa-float animated "&gt;&lt;/i&gt;</span>  nicholas.tierney@monash.edu

May 2020

&lt;br&gt;
]


]]



&lt;div class="column transition monash-m-new delay-1s" style="clip-path:url(#swipe__clip-path);"&gt;
&lt;div class="background-image" style="background-image:url('images/large.png');background-position: center;background-size:cover;margin-left:3px;"&gt;
&lt;svg class="clip-svg absolute"&gt;
&lt;defs&gt;
&lt;clipPath id="swipe__clip-path" clipPathUnits="objectBoundingBox"&gt;
&lt;polygon points="0.5745 0, 0.5 0.33, 0.42 0, 0 0, 0 1, 0.27 1, 0.27 0.59, 0.37 1, 0.634 1, 0.736 0.59, 0.736 1, 1 1, 1 0, 0.5745 0" /&gt;
&lt;/clipPath&gt;
&lt;/defs&gt;	
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;



---
# recap

- networks

---
# Overview

- What is a regression tree?
- How is it computed?
- Deciding when its a good fit
  - rmse
- Comparison with linear models
- Using multiple variables
- Next class:
  - How a classification tree differs from a regression tree?


---
# Example

&lt;img src="lecture_10a_files/figure-html/simulated-data-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Let's predict Y using a linear model


```r
df_lm &lt;- lm(y ~ x, df)
```


&lt;img src="lecture_10a_files/figure-html/lm-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Assessing model fit

- Look at residuals
- Look at mean square error

---
# Looking at the residuals: this is bad!

&lt;img src="lecture_10a_files/figure-html/resid-1.png" width="100%" style="display: block; margin: auto;" /&gt;

It basically looks like the data!

---
# Looking at the Mean square error (MSE)

This is another way to assess a model, it is like taking the average amount of error in the model.

$$
MSE(y) = \frac{\sum_{i = 1}^{i = N} (y_i - \hat{y}_i)^2}{N}
$$
In R code:


```r
mse &lt;- function(model){
  mod_aug &lt;- augment(model)
  mod_aug %&gt;% 
    mutate(res_2 = .resid^2) %&gt;% 
    summarise(mse = mean(res_2)) %&gt;% 
    pull(mse)
}

mse(df_lm)
## [1] 3.216767
```

---
# Let's use a different model: "rpart"


```r
library(rpart)
# df_lm &lt;- lm(y~x, data=df) - similar to lm! But rpart.
df_rp &lt;- rpart(y~x, data=df)
```

--

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Comparing lm vs rpart: Predictions

&lt;img src="lecture_10a_files/figure-html/compare-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Comparing lm vs rpart: MSE


```r
# linear model
mse(df_lm)
## [1] 3.216767

# rpart model
mse(df_rp)
## [1] 0.4517498
```

--

The rpart model is much lower! 

--

We can look at the residuals plotted against the values of x to get an idea

---
# Comparing lm vs rpart: residuals


&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Comparing lm vs rpart: output


```
## 
## Call:
## lm(formula = y ~ x, data = df)
## 
## Coefficients:
## (Intercept)            x  
##      0.8806      -2.2165
```

---
# Comparing lm vs rpart: output


```
## n= 100 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 100 359.245100  0.8081071  
##    2) x&gt;=0.2775916 24  16.840100 -1.4822830  
##      4) x&gt;=0.3817438 12   3.832238 -2.0814410 *
##      5) x&lt; 0.3817438 12   4.392090 -0.8831252 *
##    3) x&lt; 0.2775916 76 176.745400  1.5313880  
##      6) x&lt; 0.1426085 61  41.562800  0.9365995  
##       12) x&gt;=-0.3999242 50  24.519860  0.7035330  
##         24) x&lt; 0.05905847 41  11.729940  0.4807175  
##           48) x&gt;=-0.1455513 25   5.653876  0.2281914 *
##           49) x&lt; -0.1455513 16   1.990829  0.8752895 *
##         25) x&gt;=0.05905847 9   1.481498  1.7185820 *
##       13) x&lt; -0.3999242 11   1.981477  1.9959930 *
##      7) x&gt;=0.1426085 15  25.842970  3.9501960 *
```

---
# So what is going on?

- A linear model asks "What line fits through these points, to minimise the error"?
- A decision tree model asks "How can I best break the data into segments, to minimize some error?" 

---
# A linear model: draws the line of best fit

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# A regression tree: segments the data to reduce mean error

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Regression trees

- Regression trees recursively partition the data, and use the average response value of each partition as the model estimate
- It is a computationally intense technique that examines all possible partitions, and choosing the BEST partition by optimizing some criteria
- For regression, with a quantitative response variable, the criteria to maximise is called ANOVA:

`$$SS_T-(SS_L+SS_R)$$`
where `\(SS_T = \sum (y_i-\bar{y})^2\)`, and `\(SS_L, SS_R\)` are the equivalent values for the two subsets created by partitioning.

---
# Break down: What is `\(SS_T = \sum (y_i-\bar{y})^2\)` ?

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Break down: What is `\(SS_T = \sum (y_i-\bar{y})^2\)` ?

&lt;img src="lecture_10a_files/figure-html/what-is-error-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right


&lt;img src="lecture_10a_files/figure-html/ssl-ssr-1-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right


&lt;img src="lecture_10a_files/figure-html/ssl-ssr-2-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right


&lt;img src="lecture_10a_files/figure-html/ssl-ssr-3-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right

&lt;img src="lecture_10a_files/figure-html/ssl-ssr-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right

&lt;img src="lecture_10a_files/figure-html/fun-ssl-ssr-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Across all values of x?




&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# And if we repeated this within each split

This is how the data is split:

&lt;img src="lecture_10a_files/figure-html/show-split-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# We can represent these splits in a tree format:


```r
library(rpart.plot)
rpart.plot(df_rp)
```

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# The model predictions with the splits

&lt;img src="lecture_10a_files/figure-html/model-pred-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Your turn: compute a regression tree

Using the small data set, manually compute a regression tree model for the data. Sketch the model.


```r
d &lt;- tibble(x=c(1, 2, 3, 4, 5), y=c(10, 12, 5, 4, 3))
d
ggplot(d, aes(x=x, y=y)) + 
  geom_???()
```



---
# Stopping rules

- Its an algorithm, and it has to know when to stop.
- Why did it stop at 7 terminal nodes?
- Stopping rules are needed, else the algorithm will keep fitting until every observation is in its own group.
- Control parameters set stopping points:
   + **minsplit**: minimum number of points in a node that algorithm is allowed to split
   + **minbucket**: minimum number of points in a terminal node
- We can also look at the change in value of `\(SS_T-(SS_L+SS_R)\)` at each split, and if the change is too *small*, stop.

---
# You can change the options to fit a different model

An re-fit, the model will change. Here we reduce the `minbucket` parameter. 


```r
df_rp_m10 &lt;- rpart(y~x, data=df, 
*                       control = rpart.control(minsplit = 2))
```


---
# This yields a (slightly) more complex model.

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Beyond one variable

- So far we have only considered cases with one explanatory variable:

```r
rpart(y ~ x)
```

- When given multiple variables, a decision tree will **only use variables that provide the best splits**
- This means that we can identify variables that are important for predicting an outcome.
- This is called "Variable importance"

---
# Variable importance

- After calculating all the potential splits, each variable is given an importance value, that is typically based on the number of times it was used in splitting, and the order in the splits
- The earlier the split, the more important the variable.
- These "importance values" are usually scaled to sum to 100
- But the numbers themselves are arbitrary
- Let's explore this in the next exercise, "10-exercise-lab-1.Rmd"

---
class: transition
# Wrapping up

---
# Strengths

- There are no parametric assumptions underlying partitioning methods
- Can handle data of unusual shapes and sizes
- Can identify unusual groups of data
- Provides a tree based graphic that is fun to interpret
- Has an efficient heuristic of handling missing values. 
- The method could be influenced by outliers, but it would be isolating them to one partition.
  
---
# Weaknesses 

- Doesn't really handle data that is linear very well
- Can require tuning parameters to get good model fit

- Also means that there is not a nice formula for the model as a result, or inference about populations available
  - E.g., You can't say things like: "For every one unit increase in weight, we expect height to increase by XX amount".

---
# Next class: 

- Classification trees
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLanguage": "r",
"highlightLines": true,
"highlightSpans": false,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%/%total%",
"navigation": {
"scroll": false,
"touch": true,
"click": false
},
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
