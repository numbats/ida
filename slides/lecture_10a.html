<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ETC1010: Introduction to Data Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Nicholas Tierney &amp; Stuart Lee" />
    <script src="libs/header-attrs-2.1/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
    <link href="libs/font-awesome-animation-1.0/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome-5.0.13/js/fontawesome-all.min.js"></script>
    <!--
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> 
    -->
    <link rel="icon" href="images/favicon.ico"  type='image/x-icon'/>    
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-logo.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="assets/demo.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

  



```r
draw_split &lt;- function(split){

df_add_sst &lt;- df %&gt;% 
  mutate(x_split = if_else(x &lt;= split, "left", "right")) %&gt;% 
  group_by(x_split) %&gt;% 
  mutate(y_bar = mean(y),
         y_slope = 0) 

ggplot(df_add_sst, 
       aes(x = x, 
           y = y)) + 
  geom_point() +
  geom_vline(xintercept = split, linetype = 2) +
  geom_line(aes(y = y_bar), colour = "salmon") + 
  geom_segment(mapping = aes(xend = x, 
                             yend = y_bar), 
               color = "grey50",
               alpha = 0.5) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, 0.1))
}
```

  


&lt;!-- background-color: #006DAE --&gt;
&lt;!-- class: middle center hide-slide-number --&gt;

&lt;div class="shade_black"  style="width:60%;right:0;bottom:0;padding:10px;border: dashed 4px white;margin: auto;"&gt;
&lt;i class="fas fa-exclamation-circle"&gt;&lt;/i&gt; These slides are viewed best by Chrome and occasionally need to be refreshed if elements did not load properly. See &lt;a href=/&gt;here for PDF &lt;i class="fas fa-file-pdf"&gt;&lt;/i&gt;&lt;/a&gt;.
&lt;/div&gt;

&lt;br&gt;

.white[Press the **right arrow** to progress to the next slide!]

---

background-image: url(images/bg1.jpg)
background-size: cover
class: hide-slide-number split-70 title-slide
count: false

.column.shade_black[.content[

&lt;br&gt;

# .monash-blue.outline-text[ETC1010: Introduction to Data Analysis]

&lt;h2 class="monash-blue2 outline-text" style="font-size: 30pt!important;"&gt;Week 10, part A&lt;/h2&gt;

&lt;br&gt;

&lt;h2 style="font-weight:900!important;"&gt;Regression and Decision Trees&lt;/h2&gt;

.bottom_abs.width100[

Lecturer: *Nicholas Tierney &amp; Stuart Lee*

Department of Econometrics and Business Statistics

<span>&lt;i class="fas  fa-envelope faa-float animated "&gt;&lt;/i&gt;</span>  nicholas.tierney@monash.edu

May 2020

&lt;br&gt;
]


]]



&lt;div class="column transition monash-m-new delay-1s" style="clip-path:url(#swipe__clip-path);"&gt;
&lt;div class="background-image" style="background-image:url('images/large.png');background-position: center;background-size:cover;margin-left:3px;"&gt;
&lt;svg class="clip-svg absolute"&gt;
&lt;defs&gt;
&lt;clipPath id="swipe__clip-path" clipPathUnits="objectBoundingBox"&gt;
&lt;polygon points="0.5745 0, 0.5 0.33, 0.42 0, 0 0, 0 1, 0.27 1, 0.27 0.59, 0.37 1, 0.634 1, 0.736 0.59, 0.736 1, 1 1, 1 0, 0.5745 0" /&gt;
&lt;/clipPath&gt;
&lt;/defs&gt;	
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;



---
# recap

- networks

---
# Overview

- What is a regression tree?
- How is it computed?
- Deciding when its a good fit
  - rmse
- Comparison with linear models
- Using multiple variables
- Next class:
  - How a classification tree differs from a regression tree?


---
# Example

&lt;img src="lecture_10a_files/figure-html/simulated-data-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Let's predict Y using a linear model


```r
df_lm &lt;- lm(y ~ x, df)
```


&lt;img src="lecture_10a_files/figure-html/lm-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Assessing model fit

- Look at residuals
- Look at mean square error

---
# Looking at the residuals: this is bad!

&lt;img src="lecture_10a_files/figure-html/resid-1.png" width="100%" style="display: block; margin: auto;" /&gt;

It basically looks like the data!

---
# Looking at the Mean square error (MSE)

$$
MSE(y) = \frac{\sum_{i = 1}^{i = N} (y_i - \hat{y}_i)^2}{N}
$$
In R code:


```r
# calculate example of linear model MSE
```


---
# Let's use a different model: "rpart"


```r
library(rpart)
# df_lm &lt;- lm(y~x, data=df) - similar to lm! But rpart.
df_rp &lt;- rpart(y~x, data=df)
```


&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Look at residuals


```r
ggplot(df_rp_aug,
       aes(x = x,
           y = y)) + 
  geom_point() +
  geom_line(aes(y = .fitted), colour = "salmon", size = 2)
```

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Look at MSE

---
# Comparing lm vs rpart


&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# What the output of a linear model looks like


```
## 
## Call:
## lm(formula = y ~ x, data = df)
## 
## Coefficients:
## (Intercept)            x  
##      0.8806      -2.2165
```

---
# What the output of a rpart looks like


```
## n= 100 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 100 359.245100  0.8081071  
##    2) x&gt;=0.2775916 24  16.840100 -1.4822830  
##      4) x&gt;=0.3817438 12   3.832238 -2.0814410 *
##      5) x&lt; 0.3817438 12   4.392090 -0.8831252 *
##    3) x&lt; 0.2775916 76 176.745400  1.5313880  
##      6) x&lt; 0.1426085 61  41.562800  0.9365995  
##       12) x&gt;=-0.3999242 50  24.519860  0.7035330  
##         24) x&lt; 0.05905847 41  11.729940  0.4807175  
##           48) x&gt;=-0.1455513 25   5.653876  0.2281914 *
##           49) x&lt; -0.1455513 16   1.990829  0.8752895 *
##         25) x&gt;=0.05905847 9   1.481498  1.7185820 *
##       13) x&lt; -0.3999242 11   1.981477  1.9959930 *
##      7) x&gt;=0.1426085 15  25.842970  3.9501960 *
```

---
# Predictions from linear model vs rpart

&lt;img src="lecture_10a_files/figure-html/show-preds-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# So what is going on?

- A linear model asks "What line fits through these points, to minimise the error"?
- A decision tree model asks "How can I best break the data into segments, to minimize some error?" 

---
# A linear model: draws the line of best fit

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# A regression tree: segments the data to reduce some error

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Regression trees

- Regression trees recursively partition the data, and use the average response value of each partition as the model estimate
- It is a computationally intense technique that examines all possible partitions, and choosing the BEST partition by optimizing some criteria
- For regression, with a quantitative response variable, the criteria is called ANOVA:

`$$SS_T-(SS_L+SS_R)$$`
where `\(SS_T = \sum (y_i-\bar{y})^2\)`, and `\(SS_L, SS_R\)` are the equivalent values for the two subsets created by partitioning.

---
# Break down: What is `\(SS_T = \sum (y_i-\bar{y})^2\)` ?

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Break down: What is `\(SS_T = \sum (y_i-\bar{y})^2\)` ?

&lt;img src="lecture_10a_files/figure-html/what-is-error-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right


&lt;img src="lecture_10a_files/figure-html/ssl-ssr-1-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right


&lt;img src="lecture_10a_files/figure-html/ssl-ssr-2-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right


&lt;img src="lecture_10a_files/figure-html/ssl-ssr-3-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right

&lt;img src="lecture_10a_files/figure-html/ssl-ssr-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# `\(SS_L\)` `\(SS_R\)`? Choose a point, compare the left and right

&lt;img src="lecture_10a_files/figure-html/fun-ssl-ssr-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Across all values of x?




&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# And if we repeated this again

This is how the data is split:

&lt;img src="lecture_10a_files/figure-html/show-split-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# We can represent these splits in a tree format:


```r
library(rpart.plot)
rpart.plot(df_rp)
```

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Your turn: compute a regression tree

Using the small data set, manually compute a regression tree model for the data. Sketch the model.


```r
d &lt;- tibble(x=c(1, 2, 3, 4, 5), y=c(10, 12, 5, 4, 3))
d
ggplot(d, aes(x=x, y=y)) + 
  geom_???()
```



---
# Understanding rpart


```r
df_rp
## n= 100 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 100 359.245100  0.8081071  
##    2) x&gt;=0.2775916 24  16.840100 -1.4822830  
##      4) x&gt;=0.3817438 12   3.832238 -2.0814410 *
##      5) x&lt; 0.3817438 12   4.392090 -0.8831252 *
##    3) x&lt; 0.2775916 76 176.745400  1.5313880  
##      6) x&lt; 0.1426085 61  41.562800  0.9365995  
##       12) x&gt;=-0.3999242 50  24.519860  0.7035330  
##         24) x&lt; 0.05905847 41  11.729940  0.4807175  
##           48) x&gt;=-0.1455513 25   5.653876  0.2281914 *
##           49) x&lt; -0.1455513 16   1.990829  0.8752895 *
##         25) x&gt;=0.05905847 9   1.481498  1.7185820 *
##       13) x&lt; -0.3999242 11   1.981477  1.9959930 *
##      7) x&gt;=0.1426085 15  25.842970  3.9501960 *
```

**There's a lot going on here, let's break it down.**


---
# This is how the model looks:

&lt;img src="lecture_10a_files/figure-html/model-pred-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Stopping rules

- Its an algorithm. Why did it stop at 7 terminal nodes?
- Stopping rules are needed, else the algorithm will keep fitting until every observation is in its own group.
- Control parameters set stopping points:
   + minsplit: minimum number of points in a node that algorithm is allowed to split
   + minbucket: minimum number of points in a terminal node
- We can also look at the change in value of `\(SS_T-(SS_L+SS_R)\)` at each split, and if the change is too *small*, stop.

---
# You can change the options to fit a different model

An re-fit, the model will change. Here we reduce the `minbucket` parameter. 


```r
df_rp_m10 &lt;- rpart(y~x, data=df, 
*                       control = rpart.control(minsplit = 2))
```


---
# This yields a (slightly) more complex model.

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# What's computed?




&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-18-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Residuals


```r
ggplot(df_rp_aug, aes(x=x, y= .resid)) + geom_point() +
  ylab("residuals") + scale_x_continuous(breaks=seq(-0.5, 0.5, 0.1))
```

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-19-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Goodness of fit - Root Mean Square Error


```r
gof &lt;- printcp(df_rp, digits=3)
## 
## Regression tree:
## rpart(formula = y ~ x, data = df)
## 
## Variables actually used in tree construction:
## [1] x
## 
## Root node error: 359/100 = 3.59
## 
## n= 100 
## 
##       CP nsplit rel error xerror   xstd
## 1 0.4611      0     1.000  1.008 0.1426
## 2 0.3044      1     0.539  0.580 0.0891
## 3 0.0419      2     0.235  0.301 0.0643
## 4 0.0315      3     0.193  0.248 0.0621
## 5 0.0240      4     0.161  0.246 0.0623
## 6 0.0114      5     0.137  0.218 0.0617
## 7 0.0100      6     0.126  0.216 0.0616
```

---
# goodness of fit?

The relative error is `\(1-R^2\)`. For this example, after 6 splits it is 0.1371214. So `\(R^2=\)` 0.8628786. 


```r
1-sum(df_rp_aug$e^2)/sum((df$y-mean(df$y))^2)
```

---
# Strengths

  - There are no parametric assumptions underlying partitioning methods
  - Can handle data of unusual shapes and sizes?
  - Can identify unusual groups of data
  - Provides a tree based graphic that is fun to interpret
  - Has an efficient heuristic of handling missing values. 
  - The method could be influenced by outliers, but it would be isolating the effect to one partition.
  
---
# Weaknesses 

  - Doesn't really handle data that is linear very well
  - Can require tuning parameters to get good model fit

- Also means that there is not a nice formula for the model as a result, or inference about populations available

---
# Next week: Classification trees

When the response is categorical, the model is called a classification tree. The criteria for making the splits changes also. There are a number of split criteria commonly used. If we consider a binary response ($y=0, 1$), and `\(p\)` is the proportion of observations in class `\(1\)`.

- Gini: `\(2p(1-p)\)`
- Entropy: `\(-p(\log_e p)-(1-p)\log_e(1-p)\)`

Which rewards splits where the observations are all one class.

---
# Your Turn: Lab exercise

- Return of the paintings data
- Just predict price with year


```r
pp &lt;- read_csv(here::here("slides/data/paris-paintings.csv"))

pp_lm &lt;- lm(logprice ~ Height_in + Width_in, data = pp)
pp_rp &lt;- rpart(logprice ~ Height_in + Width_in + year, data = pp)

pp_lm_aug &lt;- augment(pp_lm)
pp_rp_aug &lt;- augment(pp_rp)
library(rpart.plot)
rpart.plot(pp_rp)
```

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-22-1.png" width="100%" style="display: block; margin: auto;" /&gt;

- include a bit on adding more variables
- include a bit on using more variables to predict something interesting
- link to the data dictionary: http://www2.stat.duke.edu/~cr173/Sta112_Fa16/data/paris_paintings.html
- describe why we've subsetted the data (to avoid perfect predictors?)


```r
pp_rp_all &lt;- rpart(logprice ~ ., data = select(pp, 
                                               logprice,
                                               origin_author,
                                               school_pntg,
                                               artistliving,
                                               authorstyle,
                                               endbuyer,
                                               Height_in:mat,
                                               engraved:finished,
                                               relig,
                                               landsALL,
                                               arch:other))

rpart.plot(pp_rp_all)
```

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-23-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---


```r
tidy(pp_rp_all) %&gt;% 
  ggplot(aes(x = importance,
             y = reorder(variable, importance))) + 
  geom_col()
```

&lt;img src="lecture_10a_files/figure-html/unnamed-chunk-24-1.png" width="100%" style="display: block; margin: auto;" /&gt;

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLanguage": "r",
"highlightLines": true,
"highlightSpans": false,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%/%total%",
"navigation": {
"scroll": false,
"touch": true,
"click": false
},
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
