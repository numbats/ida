<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ETC1010: Introduction to Data Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Professer Di Cook &amp; Nicholas Tierney &amp; Stuart Lee" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
    <!--
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> 
    -->
    <link rel="icon" href="images/favicon.ico"  type='image/x-icon'/>    
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-logo.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="assets/demo.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

















&lt;!-- background-color: #006DAE --&gt;
&lt;!-- class: middle center hide-slide-number --&gt;

&lt;div class="shade_black"  style="width:60%;right:0;bottom:0;padding:10px;border: dashed 4px white;margin: auto;"&gt;
&lt;i class="fas fa-exclamation-circle"&gt;&lt;/i&gt; These slides are viewed best by Chrome and occasionally need to be refreshed if elements did not load properly. See &lt;a href=/&gt;here for PDF &lt;i class="fas fa-file-pdf"&gt;&lt;/i&gt;&lt;/a&gt;.
&lt;/div&gt;

&lt;br&gt;

.white[Press the **right arrow** to progress to the next slide!]

---

background-image: url(images/bg1.jpg)
background-size: cover
class: hide-slide-number split-70 title-slide
count: false

.column.shade_black[.content[

&lt;br&gt;

# .monash-blue.outline-text[ETC1010: Introduction to Data Analysis]

&lt;h2 class="monash-blue2 outline-text" style="font-size: 30pt!important;"&gt;Week 10, part B&lt;/h2&gt;

&lt;br&gt;

&lt;h2 style="font-weight:900!important;"&gt;Classification Trees&lt;/h2&gt;

.bottom_abs.width100[

Lecturer: *Professer Di Cook &amp; Nicholas Tierney &amp; Stuart Lee*

Department of Econometrics and Business Statistics

<span>&lt;i class="fas  fa-envelope faa-float animated "&gt;&lt;/i&gt;</span>  nicholas.tierney@monash.edu

May 2020

&lt;br&gt;
]


]]



&lt;div class="column transition monash-m-new delay-1s" style="clip-path:url(#swipe__clip-path);"&gt;
&lt;div class="background-image" style="background-image:url('images/large.png');background-position: center;background-size:cover;margin-left:3px;"&gt;
&lt;svg class="clip-svg absolute"&gt;
&lt;defs&gt;
&lt;clipPath id="swipe__clip-path" clipPathUnits="objectBoundingBox"&gt;
&lt;polygon points="0.5745 0, 0.5 0.33, 0.42 0, 0 0, 0 1, 0.27 1, 0.27 0.59, 0.37 1, 0.634 1, 0.736 0.59, 0.736 1, 1 1, 1 0, 0.5745 0" /&gt;
&lt;/clipPath&gt;
&lt;/defs&gt;	
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;



---
# recap

- Decision Tree

---
# Admin

- Project
  - Use of data
  - Don't use kaggle datasets
  - Talk to us about your data in class and at consults
- Practical exam
  - Next Wednesday from 12pm Wednesday, closing 12pm Thursday

---
# What is a decision tree?

.pull-left[
Tree based models consist of one or more of nested `if-then` statements for the predictors that partition the data. Within these partitions, a model is used to predict the outcome.
]

.pull-right[
&lt;img src="images/tree.jpg" width="100%" style="display: block; margin: auto;" /&gt;

.small[Source: [Egor Dezhic](becominghuman.ai)]

]


---
# Regression Tree

.pull-left[
&lt;img src="lecture_10b_files/figure-html/reg-tree-split-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="lecture_10b_files/figure-html/show-split-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]


---
# Regression Tree

.pull-left[
&lt;img src="lecture_10b_files/figure-html/show-split-again-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="lecture_10b_files/figure-html/rpart-plot-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Regression tree

- What if we want to predict something being in a particular group? Say,predicting whether someone passes a course based on two exam scores:
- Moving from continuous to categorical response.

&lt;img src="lecture_10b_files/figure-html/unnamed-chunk-1-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---
# Regression? Classification?
  
- Regression trees give the predicted response for an observation by using the mean response of the observations that belong to the
same terminal node:

&lt;img src="lecture_10b_files/figure-html/show-reg-pred-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---
# Classification

A classification tree predicts each observation belonging to the most commonly occurring class of observations.

However, when we interpret a classification tree, we are often interested not only in the class prediction (what is most common), but also the proportion of correct classifications.

---
# Building a classification tree

- Similar approach to building a classification tree as for regression trees
- We use this "recursive binary splitting" approach
- But we don't use the residual sums of squares


$$
SS_T = \sum (y_i-\bar{y})^2
$$


Since we now have a category, we need some way to describe that.

We need something else!

---
# Classification tree

- We can use the "classification error".
- Where we count up the number of mis-classified things, and choose the split that has the lowest number of mis-classified things.
- We can represent this in an equation as the .orange[fraction of observations in a region which don't belong to the most common class].

  
$$
E = 1 - \text{max}_{k}(\hat{p}_{mk})
$$

Here,  `\(\hat{p}_{mk}\)` refers to the proportion of observations in the `\(m\)`th region, from the `\(k\)`th class. 

---
# Understanding classification

Another way to think about this is to understand when E is zero, and when E is large


`\(E = 1 - \text{max}_{k}(\hat{p}_{mk})\)`


E is zero when `\(\text{max}_{k}(\hat{p}_{mk})\)` is 1, which is 1 when observations are the same class:



---
# Classification trees

- A classification tree is used to predict a .orange[categorical response] and regression tree is used to predict a quantitative response
- Use a recursive binary splitting to grow a classification tree. That is, sequentially break the data into two subsets, typically using a single variable each time.
- The predicted value for a new observation, `\(x_0\)`, will be the .orange[most commonly occurring class] of observations in the sub-region in which `\(x_0\)` falls


---
# Predicting pass or fail ?

Consider the dataset `Exam` where two exam scores are given for each student, 
and a class `Label` represents whether they passed or failed the course.

.pull-left[

```
##      Exam1    Exam2 Label
## 1 34.62366 78.02469     0
## 2 30.28671 43.89500     0
## 3 35.84741 72.90220     0
## 4 60.18260 86.30855     1
```
]

.pull-right[
&lt;img src="lecture_10b_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---
# Your turn: 

Open "10b-exercise-intro.Rmd" and let's decide a point to split the data.

---
# Calculate the number of misclassifications

Along all splits for `Exam1` classifying according to the majority class for the left and right splits
 
&lt;img src="gifs/two_d_cart.gif" width="80%" style="display: block; margin: auto;" /&gt;

Red dots are .orange["fails"], blue dots are .green["passes"], and crosses indicate misclassifications. .small[Source: John Ormerod, U.Syd]

---
# Calculate the number of misclassifications

Along all splits for `Exam2` classifying according to the majority class for the top and bottom splits

&lt;img src="gifs/two_d_cart2.gif" width="80%" style="display: block; margin: auto;" /&gt;

Red dots are .orange["fails"], blue dots are .green["passes"], and crosses indicate misclassifications. .small[Source: John Ormerod, U.Syd]

---
# Combining the results from `Exam1` and `Exam2` splits

- The minimum number of misclassifications from using all possible splits of `Exam1` was 19 when the value of `Exam1` was **56.7**
- The minimum number of misclassifications from using all possible splits of `Exam2` was 23 when the value of `Exam2` was .orange[52.5]

--

So we split on the best of these, i.e., split the data on `Exam1` at 56.7.
---
# Split criteria - purity/impurity metrics

It turns out that classification error is not sufficiently sensitive for tree-growing.

In practice two other measures are preferable, as they are more sensitive:

- The Gini Index and 
- Information Entropy. 

They are both quite similar numerically. 

Small values mean that a node contains mostly observations of a single class, referred to as .orange[node purity].

---
# Example - predicting heart disease

`\(Y\)`: presence of heart disease (Yes/No)

`\(X\)`: heart and lung function measurements


```
##  [1] "Age"       "Sex"       "ChestPain" "RestBP"    "Chol"      "Fbs"      
##  [7] "RestECG"   "MaxHR"     "ExAng"     "Oldpeak"   "Slope"     "Ca"       
## [13] "Thal"      "AHD"
```


&lt;img src="lecture_10b_files/figure-html/rpart-heart-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---
# Deeper trees

Trees can be built deeper by:

- decreasing the value of the complexity parameter `cp`, which sets the difference between impurity values required to continue splitting.
- reducing  the `minsplit` and `minbucket` parameters,  which control the number of  observations  below splits are forbidden.

&lt;img src="lecture_10b_files/figure-html/deeper-trees-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---
# Tabulate true vs predicted to make a .orange[confusion table]. 

&lt;center&gt;
&lt;table&gt;
&lt;tr&gt;  &lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt; &lt;td colspan="2" align="center" &gt; true &lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt;  &lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt; &lt;td align="center" bgcolor="#daf2e9" width="80px"&gt; C1 (positive) &lt;/td&gt; &lt;td align="center" bgcolor="#daf2e9" width="80px"&gt; C2 (negative) &lt;/td&gt; &lt;/tr&gt;
&lt;tr height="50px"&gt;  &lt;td&gt; pred- &lt;/td&gt;&lt;td bgcolor="#daf2e9"&gt; C1 &lt;/td&gt; &lt;td align="center" bgcolor="#D3D3D3"&gt; &lt;em&gt;a&lt;/em&gt; &lt;/td&gt; &lt;td align="center" bgcolor="#D3D3D3"&gt; &lt;em&gt;b&lt;/em&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;tr height="50px"&gt;  &lt;td&gt;icted &lt;/td&gt;&lt;td bgcolor="#daf2e9"&gt; C2&lt;/td&gt; &lt;td align="center" bgcolor="#D3D3D3"&gt; &lt;em&gt;c&lt;/em&gt; &lt;/td&gt; &lt;td align="center" bgcolor="#D3D3D3"&gt; &lt;em&gt;d&lt;/em&gt; &lt;/td&gt; &lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;

- .orange[Accuracy: *(a+d)/(a+b+c+d)*]
- .orange[Error: *(b+c)/(a+b+c+d)*]
- Sensitivity: *a/(a+c)*  (true positive, recall)
- Specificity: *d/(b+d)* (true negative)
- .orange[Balanced accuracy: *(sensitivity+specificity)/2*]

---
# Confusion and error


```
##           Reference
## Prediction No Yes
##        No  75   5
##        Yes 11  58
##  Accuracy 
## 0.8926174
```






---
# Example - Crabs

Physical measurements on WA crabs, males and females.

.small[*Data source*: Campbell, N. A. &amp; Mahon, R. J. (1974)]

&lt;img src="lecture_10b_files/figure-html/read-crabs-1.png" width="50%" style="display: block; margin: auto;" /&gt;

---
# Example - Crabs

&lt;img src="lecture_10b_files/figure-html/crabs-plot-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
# Comparing models

.pull-left[

Classification tree

&lt;img src="lecture_10b_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]


.pull-right[

Linear discriminant classifier

&lt;img src="lecture_10b_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
# Strengths and Weaknesses

Strengths:

- The decision rules provided by trees are very easy to explain, and follow. A simple classification model.
- Trees can handle a mix of predictor types, categorical and quantitative.
- Trees efficiently operate when there are missing values in the predictors.

Weaknesses:

- Algorithm is greedy, a better final solution might be obtained by taking a second best split earlier.
- When separation is in linear combinations of variables trees struggle to provide a good classification

---
# üë©‚Äçüíª Made by a human with a computer

- Slides inspired by [https://iml.numbat.space](https://iml.numbat.space), [https://github.com/numbats/iml](https://github.com/numbats/iml).
- Created using [R Markdown](https://rmarkdown.rstudio.com) with flair by [**xaringan**](https://github.com/yihui/xaringan), and [**kunoichi** (female ninja) style](https://github.com/emitanaka/ninja-theme).

&lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLanguage": "r",
"highlightLines": true,
"highlightSpans": false,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%/%total%",
"navigation": {
"scroll": false,
"touch": true,
"click": false
},
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
