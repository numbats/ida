---
title: "ETC1010: Introduction to Data Analysis"
week: "Week 8, part B"
subtitle: "Text analysis and linear models"
author: "Nicholas Tierney"
email: "nicholas.tierney@monash.edu"
date: "May 2020"
pdflink: ""
bgimg: "images/bg1.jpg"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/animate.css"
      - "assets/monash-logo.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/styles.css" # small improvements
      - "assets/custom.css" # add your own CSS here!
      - "assets/demo.css" # this should be removed
    self_contained: false 
    seal: false 
    chakra: 'libs/remark-latest.min.js'
    lib_dir: libs
    includes:
      in_header: "assets/custom.html"
    mathjax: "assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: github 
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---
  
```{r titleslide, child="components/titleslide.Rmd"}
```


```{r setup, include=FALSE}
library(emo)
library(tidyverse)
library(tidytext)
library(knitr)
library(lubridate)
library(gridExtra)
library(plotly)
library(broom)
opts_chunk$set(echo = TRUE,   
               message = FALSE,
               warning = FALSE,
               collapse = TRUE,
               fig.height = 5,
               fig.width = 8,
               fig.retina = 2,
               fig.align = "center",
               out.width = "100%",
               cache = FALSE)

as_table <- function(...) knitr::kable(..., format='html', digits = 3)
```

```{r load-reviews, echo = FALSE}


acnh_user_reviews <- read_tsv(here::here("slides/data/acnh_user_reviews.tsv"))

acnh_user_reviews_parsed <- acnh_user_reviews %>% 
  mutate(text = str_remove(text, "Expand$"))

user_reviews_words <- acnh_user_reviews_parsed %>%
  unnest_tokens(output = word, input = text)
```

---
class: refresher

# Recap 

- tidying up text
- unnest_tokens
- stop words - (I, am, be, the, this, what, we, myself)
- sentiment analysis

---

# Upcoming Assessment

- Project
- Practical Exam
- Final Exam

---

# Project

- Complete ED quiz before Thursday
- Focus on narrowing down some interesting questions and datasets

---

# Practice Exams

- Practice exams are up for the final exam and the practical exam


---
# Overview

- Tidy Text continued
- Term Frequency
- Inverse Document Frequency
- More practice

---
# What is a document about?

## How do we measure the importance of a word to a document in a collection of documents?

i.e a novel in a collection of novels or a review in a set of reviews...

We combine the following statistics:

- Term frequency
- Inverse document frequency

---

# Term frequency

The raw frequency of a word $w$ in a document $d$. It is a function of the word and the document. 

$$tf(w, d) = \frac{\text{count of w in d}}{\text{total count in d}} $$

---
# Harry Potter books

Using data from Harry potter:

```{r read-harry-potter, echo = FALSE}
# harry potter package from
# https://github.com/bradleyboehmke/harrypotter
hp_books <- 
map_dfr(
  .x = list(
    "Philosopher's Stone" = harrypotter::philosophers_stone,
    "Chamber of Secrets" = harrypotter::chamber_of_secrets,
    "Prisoner of Azkaban" = harrypotter::prisoner_of_azkaban,
    "Goblet of Fire" = harrypotter::goblet_of_fire,
    "Order of the Phoenix" = harrypotter::order_of_the_phoenix,
    "Half-Blood Prince" = harrypotter::half_blood_prince,
    "Deathly Hallows" = harrypotter::deathly_hallows
    ),
  .f = function(x) tibble(x) %>% setNames("text"),
  .id = "book"
) %>% 
  mutate(book = as_factor(book))

hp_books

# write_rds(hp_books, here::here("slides/data/harry-potter.rds"), compress = "xz")
```

---

# Harry Potter books

Unnest tokens, and use `count` to count up the words within each book:

.pull-left[
```{r}
book_words <- hp_books %>% 
  unnest_tokens(word, text) %>% 
  count(book, word, sort = TRUE)
```
]

.pull-right[
```{r}
book_words
```
]

---
# Term frequency

Let's calculate frequency of words for The Philosopher's Stone

```{r doc-example}
stopwords_smart <- get_stopwords(source = "smart")

document <- book_words %>% 
  anti_join(stopwords_smart) %>% 
  filter(book == "Philosopher's Stone") 
document
```


---
# Term frequency

The term frequency for each word is the number of times that word occurs
divided by the total number of words in the document.

```{r tf}
tbl_tf <- document %>% 
  mutate(tf = n / sum(n))

tbl_tf %>% 
  arrange(desc(tf))
```

---
# Inverse-document frequency

We can instead look at a term's inverse document frequency (idf), which:

- Decreases weight for commonly used words, while
- Increasing weight for those words not used much in a collection of documents.

This effectively tells us **how common or rare a word is accross a collection of documents**. 

It is a function of a word $w$, and the collection of documents $\mathcal{D}$.

$$
idf(w, \mathcal{D}) = \log{\left(\frac{\text{Number of } \mathcal{D}}{\text{Number of documents containing } w}\right)}
$$

---
# Inverse-document frequency: Example

Let's say that we had 20 documents:

- Out of 20 documents $\mathcal{D}$ 
- How many documents contain the word, "the". (All 20 contain "the")

$$
idf(w = 20, \mathcal{D} = 20) = \log{\left(\frac{20}{20}\right)}
$$

$$
idf(w = 20, \mathcal{D} = 20) = \log{\left(1\right)}
$$

$$
idf(w = 20, \mathcal{D} = 20) = 0
$$

---
# Inverse-document frequency: Example

Let's say that we had 20 documents:

- Out of 20 documents $\mathcal{D}$ 
- How many documents contain the word, "Deciduous". (Only 1 contains the word "Deciduous")

$$
idf(w = 1, \mathcal{D} = 20) = \log{\left(\frac{20}{1}\right)}
$$

$$
idf(w = 1, \mathcal{D} = 20) = \log{\left(20\right)}
$$

$$
idf(w = 1, \mathcal{D} = 20) = 2.995
$$

---
# Inverse-document frequency: Example

Let's say that we had 20 documents:

- Out of 20 documents $\mathcal{D}$ 
- How many documents contain the word, "Banana". (10 contain the word "Banana")

$$
idf(w = 10, \mathcal{D} = 20) = \log{\left(\frac{20}{10}\right)}
$$

$$
idf(w = 10, \mathcal{D} = 20) = \log{\left(2\right)}
$$

$$
idf(w = 1, \mathcal{D} = 2) = 0.693
$$

---

# Inverse document frequency

- When it is higher: Word is not used much in a collection of documents
  - E.g., 1 document uses "deciduous"
- When it is lower: Word is not commonly used much in a collection of documents
  - E.g., all documents use "the", not as many use "bananas"

---

# Inverse document frequency

For the Harry Potter books, we could
compute this in a somewhat roundabout as follows:

```{r idf}
tbl_idf <- book_words %>% 
    anti_join(stopwords_smart) %>%
    mutate(collection_size = n_distinct(book)) %>% 
    group_by(collection_size, word) %>% 
    summarise(times_word_used = n_distinct(book)) %>% 
    mutate(freq = collection_size / times_word_used,
           idf = log(freq)) 
arrange(tbl_idf, idf)
```

---
# Putting it together 

**term frequency, inverse document frequency**

Multiply tf and idf together. This is a function of a word $w$, a document $d$,
and the collection of documents $\mathcal{D}$:

$$ tf\_idf(w, d, \mathcal{D}) = tf(w,d) \times idf(w, \mathcal{D})$$
- A **High value** of $tf\_idf$ means a word has a high frequency within a document
but is quite rare over all documents.
- Likewise if a word occurs in a lot of documents idf will be close to zero, so $tf\_idf$ will be small.

---
# TF IDF summary

- TF IDF  helps us find those words that are important in the content of documents
- It does this by increasing the weight of words not used very much in a collection, since the IDF is higher when a word isn't used often.
- So a **higher** TF IDF means the word is more important if it is both used a lot (has a high term frequency), and is uncommon (higher IDF).
- And a **lower** TF IDF means the word is less important, since it might be really common (high term frequency), but be really common (lower IDF).

---
# Putting it together, tf-idf 

We can calculate TF IDF using `bind_tf_idf()`

```{r calc-tf-idf}
book_words_counts <- book_words %>%
      anti_join(stopwords_smart) %>% 
      bind_tf_idf(term = word, document = book, n = n)

book_words_counts
```

---

# What words were important to the books?

```{r gg-tf-idf, echo=FALSE,message=FALSE}
book_words_counts %>% 
  group_by(book) %>%
  top_n(10, wt = tf_idf) %>%
  ungroup() %>%
  ggplot(aes(x = fct_reorder(word, tf_idf), 
             y = tf_idf, 
             fill = book)) +
  geom_col(show.legend = FALSE) + 
  coord_flip() +
  facet_wrap(~book, scales = "free") +
  scale_y_continuous() +
  theme_minimal() +
  labs(x = NULL, 
       y = "tf-idf")
```

---
class: transition

# Your Turn

Explore uncommon / important words in Jane Austen's books!
- Complete "8b-jane-austen-tf-idf.Rmd"

---
class: transition
# Sentiment analysis

Sentiment analysis tags words or phrases with an emotion, and summarises these, often as the positive or negative state, over a body of text. 

---
# Sentiment analysis: examples

- Examining effect of emotional state in twitter posts
- Determining public reactions to government policy, or new product releases
- Trying to make money in the stock market by modeling social media posts on listed companies
- Evaluating product reviews on Amazon, restaurants on zomato, or travel options on TripAdvisor

---
# Lexicons

The `tidytext` package has a lexicon of sentiments, based on four major sources: [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), [bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), [Loughran](https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists), [nrc](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

---
# emotion

What emotion do these words elicit in you?

- summer
- hot chips
- hug
- lose
- stolen
- smile

---
# Different sources of sentiment

- The `nrc` lexicon categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 
- The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. 
- The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

---

# Different sources of sentiment
```{r get-sentiment-afinn}
get_sentiments("afinn")
```

---
# Sentiment analysis

- Once you have a bag of words, you need to join the sentiments dictionary to the words data. 
- Particularly the lexicon `nrc` has multiple tags per word, so you may need to use an "inner_join". 
- `inner_join()` returns all rows from x where there are matching values in y, and all columns from x and y. 
- If there are multiple matches between x and y, all combination of the matches are returned.

---
# Exploring sentiment in Harry Potter

```{r print-tidy-ooks}
book_words
```

---
# Count joyful words in "Chamber of Secrets"

```{r count-joy}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

book_words %>%
  filter(book == "Chamber of Secrets") %>%
  inner_join(nrc_joy) %>%
  arrange(desc(n))
```

---
# Count joyful words in "Chamber of Secrets"

```{r echo = FALSE}
book_words %>%
  filter(book == "Chamber of Secrets") %>%
  inner_join(nrc_joy) %>%
  arrange(desc(n)) %>% 
  top_n(6, wt = n)
```


"Good" is the most common joyful word, followed by "diary", "found", and "smile". 

These make sense ... except for "diary", and "found", 

--

and ... "white" and "green" ?

---
class: transition
# Your turn: go to rstudio.cloud

Go to "8b-jane-austen-sentiment.Rmd"

- What are the most common "anger" words used in Emma?
- What are the most common "surprise" words used in Emma?

---
# Comparing lexicons

- All of the lexicons have a measure of positive or negative. 
- We can tag the words in Emma by each lexicon, and see if they agree. 

```{r compare-sentiments}
nrc_pn <- get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", 
                          "negative"))

secrets_nrc <- book_words %>%
  filter(book == "Chamber of Secrets") %>%
  inner_join(nrc_pn)

secrets_bing <- book_words %>%
  filter(book == "Chamber of Secrets") %>%
  inner_join(get_sentiments("bing")) 

secrets_afinn <- book_words %>%
  filter(book == "Chamber of Secrets") %>%
  inner_join(get_sentiments("afinn"))
```

---
# Comparing lexicons

```{r show-lexi} 
secrets_nrc
```

---
# Comparing lexicons

```{r show-lexi-afinn} 
secrets_afinn
```


---
# Comparing lexicons

```{r compare-sentiments-show}
secrets_nrc %>% 
  count(sentiment, name = "n_sentiment") %>% 
  mutate(prop_total = n_sentiment / sum(n_sentiment))
```


```{r compare-sentiments-show-bing}
secrets_bing %>% 
  count(sentiment, name = "n_sentiment") %>% 
  mutate(prop_total = n_sentiment / sum(n_sentiment))
```

---
# Comparing lexicons

```{r compare-sentiments-more}
secrets_afinn %>% 
  mutate(sentiment = ifelse(value > 0, 
                            "positive", 
                            "negative")) %>% 
  count(sentiment, name = "n_sentiment") %>% 
  mutate(prop_total = n_sentiment / sum(n_sentiment))

```

---
class: transition
# Your turn: 

Continue along with "8b-jane-austen-sentiment.Rmd"

- Using your choice of lexicon (nrc, bing, or afinn) compute the proportion of positive words in each of Austen's books.
- Which book is the most positive? negative?


---
# Example: Simpsons

Data from the popular animated TV series, The Simpsons, has been made available on [kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data/data). 

- `simpsons_script_lines.csv`: Contains the text spoken during each episode (including details about which character said it and where)
- `simpsons_characters.csv`: Contains character names and a character id

---
# The Simpsons

```{r read-scripts}
scripts <- read_csv("data/simpsons_script_lines.csv")
chs <- read_csv("data/simpsons_characters.csv")
sc <- left_join(scripts, chs, by = c("character_id" = "id"))

sc
```

---
# count the number of times a character speaks

```{r count-names}
sc %>% count(name, sort = TRUE)
```

---
# missing name?

```{r explore-missing}
sc %>% filter(is.na(name))
```


---
# Simpsons Pre-process the text

```{r process-simpsons-s1}
sc %>%
  unnest_tokens(output = word, 
                input = spoken_words)
```

---
# Simpsons Pre-process the text

```{r process-simpsons-s2}
sc %>%
  unnest_tokens(output = word, 
                input = spoken_words) %>%
  anti_join(stop_words)
```

---
# Simpsons Pre-process the text

```{r process-simpsons-s3}
sc %>%
  unnest_tokens(output = word, 
                input = spoken_words) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(!is.na(word))
```

---
# Simpsons Pre-process the text

```{r process-simpsons-s4}
sc_top_20 <- sc %>%
  unnest_tokens(output = word, 
                input = spoken_words) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(!is.na(word)) %>%
  mutate(word = factor(word, 
                       levels = rev(unique(word)))) %>%
  top_n(20)
```

---
# Simpsons plot most common words

.left-code[
```{r process-simpsons-s5, eval = FALSE}
ggplot(sc_top_20,
       aes(x = word, 
           y = n)) +
  geom_col() +
  labs(x = '', 
       y = 'count', 
       title = 'Top 20 words') +
  coord_flip() + 
  theme_bw()
```  
]


.right-plot[
```{r process-simpsons-s5-out, ref.label = 'process-simpsons-s5', echo = FALSE, out.width = "100%"}

```
]

---
# Tag the words with sentiments

Using AFINN words will be tagged on a negative to positive scale of -1 to 5.

```{r tag-sentiments}
sc_word <- sc %>%
  unnest_tokens(output = word, input = spoken_words) %>%
  anti_join(stop_words) %>%
  count(name, word) %>%
  filter(!is.na(word))

sc_word
```

---
# Tag the words with sentiments
```{r tag-sentiments2}
sc_s <- sc_word %>% 
  inner_join(get_sentiments("afinn"), by = "word")

sc_s
```

---
# Examine Simpsons characters

```{r summarise-simpsons-characters}
sc_s %>% 
  group_by(name) %>% 
  summarise(m = mean(value)) %>% 
  arrange(desc(m))
```

---
# Examine Simpsons characters: Focus characters.

```{r keep-main-chars}
keep <- sc %>% count(name, 
                     sort=TRUE) %>%
  filter(!is.na(name)) %>%
  filter(n > 999)

sc_s %>% 
  filter(name %in% keep$name) %>% 
  group_by(name) %>% 
  summarise(m = mean(value)) %>% 
  arrange(m)
```

---
class: transition
# Your turn: "8b-simpsons.Rmd"

1. Bart Simpson is featured at various ages. How has the sentiment of his words changed over his life?

2. Repeat the sentiment analysis with the NRC lexicon. What character is the most "angry"? "joyful"?

---
class: transition

# Extension: Explore Harry Potter

I've included the harry potter data the code from the harry potter part of the lecture in "8b-harry-potter.Rmd", if you want to have a play around, I've got a few questions there.

---
# Further extension

Text Mining with R has an example comparing historical physics textbooks:  *Discourse on Floating Bodies* by Galileo Galilei, *Treatise on Light* by Christiaan Huygens, *Experiments with Alternate Currents of High Potential and High Frequency* by Nikola Tesla, and *Relativity: The Special and General Theory* by Albert Einstein. All are available on the Gutenberg project. 

Work your way through the [comparison of physics books](https://www.tidytextmining.com/tfidf.html#a-corpus-of-physics-texts). It is section 3.4.

---
# Thanks

- Dr. Mine Çetinkaya-Rundel
- Dr. Julia Silge: https://github.com/juliasilge/tidytext-tutorial and
https://juliasilge.com/blog/animal-crossing/ 
- Dr. Julia Silge and Dr. David Robinson: https://www.tidytextmining.com/
